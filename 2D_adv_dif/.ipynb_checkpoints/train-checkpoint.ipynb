{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a60001a-1307-4518-8f70-93325dc1fddc",
   "metadata": {},
   "source": [
    "#### 2D Flexi Propagator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36625b3-425b-4262-9118-64f03ab5aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "import wandb\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config, load_config\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from model_io import load_model, save_model\n",
    "\n",
    "from data import load_from_path, prepare_adv_diff_dataset, AdvectionDiffussionDataset, get_train_val_test_folds, IntervalSplit, exact_solution\n",
    "\n",
    "# We we define all our model here:\n",
    "#from new_model import Encoder, Decoder, Propagator_concat as Propagator, Model, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f78679-8c8a-4a0f-ae69-b8b9b9ae98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Applied workaround for CuDNN issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd14d4-7ecd-48d6-9c07-037f621209d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val, alpha_interval_split, tau_interval_split = load_from_path(\"data\")\n",
    "print(f\"Alpha split: {alpha_interval_split},\\n Tau Split: {tau_interval_split}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099dfbc-773d-4f9a-9317-49ae0ae83736",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b290e0-9374-4e42-b1e4-0f10aa5e186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Layer for Conv2D\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, num_channels, num_groups=4):\n",
    "        super(Norm, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups, num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "# Encoder using Conv2D\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: (batch_size, 1, 256, 256)\n",
    "            nn.Conv2d(1, 32, kernel_size=2, stride=2, padding=0),  # (batch_size, 64, 128, 128)\n",
    "            nn.GELU(),\n",
    "            Norm(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0),  # (batch_size, 128, 64, 64)\n",
    "            nn.GELU(),\n",
    "            Norm(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),  # (batch_size, 256, 32, 32)\n",
    "            nn.GELU(),\n",
    "            Norm(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),  # (batch_size, 512, 16, 16)\n",
    "            nn.GELU(),\n",
    "            Norm(256),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),  # (batch_size, 512, 8, 8)\n",
    "            nn.GELU(),\n",
    "            Norm(512),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384f568-bab8-42f3-9d59-ccec04dcee74",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538dcca-8e1e-4403-907d-ddfdbc09e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Fully connected layer to transform the latent vector back to the shape (batch_size, 512, 8, 8)\n",
    "        self.fc = nn.Linear(latent_dim, 512 * 4 * 4)\n",
    "\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(256),\n",
    "\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(128),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(64),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(32),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Transform the latent vector to match the shape of the feature maps\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 512, 4, 4)  # Reshape to (batch_size, 512, 4, 4)\n",
    "        x = self.deconv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67890e4-96d7-4098-9913-4e2489210614",
   "metadata": {},
   "source": [
    "### Propagator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0344e5-496e-44aa-99fc-5f5c1307502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propagator_concat(nn.Module): \n",
    "    \"\"\"\n",
    "    Takes in (z(t), tau, alpha) and outputs z(t+tau)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, feats=[16, 32, 64]):\n",
    "        \"\"\"\n",
    "        Initialize the propagator network.\n",
    "        Input : (z(t), tau)\n",
    "        Output: z(t+tau)\n",
    "        \"\"\"\n",
    "        super(Propagator_concat, self).__init__()\n",
    "\n",
    "        self._net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 2, feats[0]),  # 1 is for tau; more params will increase this\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feats[0], feats[1]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feats[1], feats[2]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feats[2], latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, tau, alpha):\n",
    "        \"\"\"\n",
    "        Forward pass of the propagator.\n",
    "        Concatenates latent vector z with tau and processes through the network.\n",
    "        \"\"\"\n",
    "        zproj = z.squeeze(1)  # Adjust z dimensions if necessary\n",
    "        z_ = torch.cat((zproj, tau, alpha), dim=1)  # Concatenate z and tau along the last dimension\n",
    "        z_tau = self._net(z_)\n",
    "        return z_tau, z_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196df36-a070-4be6-9bb9-15f29c07ac76",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b147a3d-1bbc-4492-a2f2-1764154767cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder, propagator):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder # decoder for x(t)\n",
    "        self.propagator = propagator  # used to time march z(t) to z(t+tau)\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, tau, alpha):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "\n",
    "        # Update small fcnn to get z(t+tau) from z(t)\n",
    "        z_tau, z_ = self.propagator(z, tau, alpha)\n",
    "\n",
    "        # Reconstruction\n",
    "        x_hat = self.decoder(z)  # Reconstruction of x(t)\n",
    "        x_hat_tau = self.decoder(z_tau)\n",
    "\n",
    "        return x_hat, x_hat_tau, mean, log_var, z_tau, z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea30b3c-9366-4b4d-95fd-c568e282e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(latent_dim):\n",
    "    # Instantiate encoder, decoder, and model\n",
    "    encoder = Encoder(latent_dim)\n",
    "    decoder  = Decoder(latent_dim)  # Decoder for x(t)\n",
    "    propagator = Propagator_concat(latent_dim) # z(t) --> z(t+tau)\n",
    "    model = Model(encoder, decoder, propagator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aaa5c0-bfa6-45dd-9f56-0fe2284cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(latent_dim = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf5599-7a43-47a5-ace9-8fb4257dae2f",
   "metadata": {},
   "source": [
    "### Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33820f-aef7-4827-a3a9-bdb517cbc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_tau, x_hat, x_hat_tau, mean, log_var):\n",
    "    \"\"\"\n",
    "    Compute the VAE loss components.\n",
    "    :param x: Original input\n",
    "    :param x_tau: Future input (ground truth)\n",
    "    :param x_hat: Reconstructed x(t)\n",
    "    :param x_hat_tau: Predicted x(t+tau)\n",
    "    :param mean: Mean of the latent distribution\n",
    "    :param log_var: Log variance of the latent distribution\n",
    "    :return: reconstruction_loss1, reconstruction_loss2, KLD\n",
    "    \"\"\"\n",
    "    reconstruction_loss1 = nn.MSELoss()(x, x_hat)  # Reconstruction loss for x(t)\n",
    "    reconstruction_loss2 = nn.MSELoss()(x_tau, x_hat_tau)  # Prediction loss for x(t+tau)\n",
    "    \n",
    "    # Kullback-Leibler Divergence\n",
    "    KLD = torch.mean(-0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), dim=1))  # Updated dim\n",
    "    \n",
    "    return reconstruction_loss1, reconstruction_loss2, KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e37a54-5ca2-47c1-b864-b4e59b9eb560",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568410e-44cd-44be-b6e4-7505b9dd2a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size):\n",
    "    data = list(zip(dataset.X, dataset.X_tau, dataset.t_values, dataset.tau_values, dataset.alpha_values))\n",
    "    data = data[: len(data) - len(data) % batch_size]\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db8b37-700e-4a25-bb80-14c0f105fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Val Loader:\n",
    "train_loader = get_data_loader(dataset_train, batch_size = 32)\n",
    "val_loader = get_data_loader(dataset_val, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c60bc-0803-48e8-aec2-5a18394fb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, X_tau, t_values, tau_values, alpha_values in train_loader:\n",
    "#     print(X.shape, X_tau.shape, t_values.shape, tau_values.shape, alpha_values.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb085f8a-12fd-4087-b964-74bfa0dfbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that all ----  X, X_tau, tau_values, alpha_values --- need to be unsqueezed along dim 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e711258-1fae-4cd8-b2e8-791b8ee0000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.train().cuda()\n",
    "# for data in train_loader:\n",
    "#     x, x_tau, t_values, tau_values, alpha_values = data\n",
    "#     x, x_tau, t_values, tau_values, alpha_values  = x.unsqueeze(1).float().cuda(), x_tau.unsqueeze(1).float().cuda(), t_values.unsqueeze(1).float().cuda(), tau_values.unsqueeze(1).float().cuda(), alpha_values.unsqueeze(1).float().cuda() \n",
    "    \n",
    "#     print(\"Input data shape: \", x.shape)\n",
    "#     print(\"Shifted data shape: \", x_tau.shape)\n",
    "#     print(\"Tau shape: \", tau_values.shape)\n",
    "#     print(\"Alpha shape: \", alpha_values.shape)\n",
    "#     print()\n",
    "    \n",
    "#     x_hat, x_hat_tau, mean, log_var, z_tau, z_ = model(x, tau_values, alpha_values)\n",
    "\n",
    "#     print()\n",
    "#     print(\"Reconstruction data shape: \", x_hat.shape)\n",
    "#     print(\"Prediction data shape: \", x_hat_tau.shape)\n",
    "#     print(\"Mean shape: \", mean.shape)\n",
    "#     print(\"Logvar shape: \", log_var.shape)\n",
    "#     print(\"Z tau Shape: \", z_tau.shape)\n",
    "#     print(\"Expanded Latent Shape: \", z_.shape)\n",
    "    \n",
    "#     RL_1, RL_2, KLD = loss_function(x, x_tau, x_hat, x_hat_tau, mean, log_var) # (x, x_tau, x_hat, x_hat_tau, mean, log_var)\n",
    "#     print(f\"RL_1-> {RL_1}, RL_2 -> {RL_2}, KLD -> {KLD}\")\n",
    "#     overall_loss = RL_1 + 3.5*RL_2 + 0.00001*KLD\n",
    "#     print(\"Overall Loss: \", overall_loss)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d55dc-a95e-41ef-9ade-71b78ce4f616",
   "metadata": {},
   "source": [
    "### Simple Training Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da965748-5ad9-4030-93ce-ba8b298170a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporate the validation loop - and save the best model weights based on validation metric\n",
    "# Observe that there are only 9k samples in the validation - we need to keep it such that t is 30% train_samples -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cbbe1c-7d7a-4148-8933-44495c745409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the predictions\n",
    "def plot_prediction(x, x_tau, x_hat, x_hat_tau, tau, alpha):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(5, 4))  # 2 rows, 2 columns\n",
    "\n",
    "    # Plot each field\n",
    "    axes[0, 0].imshow(x.cpu().squeeze().numpy(), cmap=\"jet\")\n",
    "    axes[0, 0].set_title(\"x\", fontsize=12)\n",
    "\n",
    "    axes[0, 1].imshow(x_tau.cpu().squeeze().numpy(), cmap=\"jet\")\n",
    "    axes[0, 1].set_title(\"x_tau\", fontsize=12)\n",
    "\n",
    "    axes[1, 0].imshow(x_hat.cpu().squeeze().detach().numpy(), cmap=\"jet\")\n",
    "    axes[1, 0].set_title(\"x_hat\", fontsize=12)\n",
    "\n",
    "    axes[1, 1].imshow(x_hat_tau.cpu().squeeze().detach().numpy(), cmap=\"jet\")\n",
    "    axes[1, 1].set_title(\"x_hat_tau\", fontsize=12)\n",
    "\n",
    "    # Add a common title for the figure\n",
    "    fig.suptitle(f\"Tau: {tau.item()}, Re: {alpha.item():.2f}\", fontsize=12)\n",
    "\n",
    "    # Remove axes for clean visualization\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "    \n",
    "\n",
    "# def validate(model, val_loader, epoch):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     for batch in val_loader:\n",
    "#         x, x_tau, t, tau, alpha = batch\n",
    "#         x, x_tau, t, tau, alpha = x.cuda().float().unsqueeze(1), x_tau.cuda().float().unsqueeze(1), t.cuda().float().unsqueeze(1), tau.cuda().float().unsqueeze(1), alpha.cuda().float().unsqueeze(1)\n",
    "#         x_hat, x_hat_tau, mean, log_var, z_tau, _ = model(x, tau, alpha)\n",
    "#         reconstruction_loss, reconstruction_loss_tau, KLD = loss_function(x, x_tau, x_hat, x_hat_tau, mean, log_var)\n",
    "#         loss = reconstruction_loss + gamma * reconstruction_loss_tau + beta * KLD\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#     # plot the last sample\n",
    "#     if epoch%10 == 0:\n",
    "#         fig, ax = plot_prediction(x[0], x_tau[0], x_hat[0], x_hat_tau[0], tau[0], alpha[0])\n",
    "#         plt.show()  # Display the plot\n",
    "#         plt.close(fig)  # Close the figure to avoid memory issues\n",
    "        \n",
    "#     model.train()\n",
    "#     return np.mean(losses)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, epoch):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    losses = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in val_loader:\n",
    "            x, x_tau, t, tau, alpha = batch\n",
    "            x, x_tau, t, tau, alpha = (\n",
    "                x.cuda().float().unsqueeze(1),\n",
    "                x_tau.cuda().float().unsqueeze(1),\n",
    "                t.cuda().float().unsqueeze(1),\n",
    "                tau.cuda().float().unsqueeze(1),\n",
    "                alpha.cuda().float().unsqueeze(1),\n",
    "            )\n",
    "            x_hat, x_hat_tau, mean, log_var, z_tau, _ = model(x, tau, alpha)\n",
    "            reconstruction_loss, reconstruction_loss_tau, KLD = loss_function(\n",
    "                x, x_tau, x_hat, x_hat_tau, mean, log_var\n",
    "            )\n",
    "            loss = reconstruction_loss + gamma * reconstruction_loss_tau + beta * KLD\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            fig, ax = plot_prediction(x[0], x_tau[0], x_hat[0], x_hat_tau[0], tau[0], alpha[0])\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dbda9-c747-4f6c-801c-fe1a5bf20039",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 31\n",
    "gamma = 3.0\n",
    "beta = 1e-4\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, scheduler, and loss trackers\n",
    "model = model.train().cuda()  # Putting model on GPU and setting it to train mode\n",
    "train_losses = []  # To track the training loss after each epoch\n",
    "val_losses = []    # To track the validation loss after each epoch\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4) # Optimizer with learning rate\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)  # Scheduler to adjust learning rate\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set model to train mode\n",
    "    train_epoch_loss = 0.0  # Accumulate training loss for the epoch\n",
    "    for data in train_loader:\n",
    "        x, x_tau, t_values, tau_values, alpha_values = data\n",
    "        x, x_tau, t_values, tau_values, alpha_values = (\n",
    "            x.unsqueeze(1).float().cuda(),\n",
    "            x_tau.unsqueeze(1).float().cuda(),\n",
    "            t_values.unsqueeze(1).float().cuda(),\n",
    "            tau_values.unsqueeze(1).float().cuda(),\n",
    "            alpha_values.unsqueeze(1).float().cuda(),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "        \n",
    "        # Forward pass\n",
    "        x_hat, x_hat_tau, mean, log_var, z_tau, z_ = model(x, tau_values, alpha_values)\n",
    "        \n",
    "        # Compute loss\n",
    "        RL_1, RL_2, KLD = loss_function(x, x_tau, x_hat, x_hat_tau, mean, log_var)\n",
    "        overall_loss = RL_1 + gamma * RL_2 + beta * KLD  # Weighted Loss Function\n",
    "        \n",
    "        # Backward pass\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss\n",
    "        train_epoch_loss += overall_loss.item()\n",
    "\n",
    "    avg_train_loss = train_epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    mean_loss = validate(model, val_loader, epoch)\n",
    "\n",
    "    # Print losses for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {mean_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8509a84-4915-42b8-a4d5-c7cf15174fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312ba5a-d8b6-435a-9062-233af67035cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcdac1-8fba-47f5-bd8b-893522473532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1826ff3-5f65-44bb-9060-3fc6d872618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of completed epochs\n",
    "completed_epochs = len(train_losses)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(range(1, completed_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, completed_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ddb36-b7ba-45cd-b589-64169e69eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    x, x_tau, t_values, tau_values, alpha_values = data\n",
    "    x, x_tau, t_values, tau_values, alpha_values  = x.unsqueeze(1).float().cuda(), x_tau.unsqueeze(1).float().cuda(), t_values.unsqueeze(1).float().cuda(), tau_values.unsqueeze(1).float().cuda(), alpha_values.unsqueeze(1).float().cuda() \n",
    "    \n",
    "    # print(\"Input data shape: \", x.shape)\n",
    "    # print(\"Shifted data shape: \", x_tau.shape)\n",
    "    # print(\"Tau shape: \", tau_values.shape)\n",
    "    # print(\"Alpha shape: \", alpha_values.shape)\n",
    "    # print()\n",
    "    \n",
    "    x_hat, x_hat_tau, mean, log_var, z_tau, z_ = model(x, tau_values, alpha_values)\n",
    "    \n",
    "    # print()\n",
    "    # print(\"Reconstruction data shape: \", x_hat.shape)\n",
    "    # print(\"Prediction data shape: \", x_hat_tau.shape)\n",
    "    # print(\"Mean shape: \", mean.shape)\n",
    "    # print(\"Logvar shape: \", log_var.shape)\n",
    "    # print(\"Z tau Shape: \", z_tau.shape)\n",
    "    # print(\"Expanded Latent Shape: \", z_.shape)\n",
    "\n",
    "\n",
    "    index = np.random.randint(0, len(x))\n",
    "    print(\"Index\", index)\n",
    "\n",
    "    plt.figure(figsize = (14, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(x[index, :, :, :].squeeze().cpu().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Truth\")\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(x_hat[index, :, :, :].squeeze().cpu().detach().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Model Reconstruction\")\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(x_tau[index, :, :, :].squeeze().cpu().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Truth: Forecast State\")\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(x_hat_tau[index, :, :, :].squeeze().cpu().detach().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Model Prediction\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34b1c7-4039-4494-9877-706cc68d7d42",
   "metadata": {},
   "source": [
    "### Validating on the val loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abb474-6450-4357-aa6a-18d60ba11db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = get_data_loader(dataset_val, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84fb5d-d644-447d-97b7-fb148f083dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in val_loader:\n",
    "    x, x_tau, t_values, tau_values, alpha_values = data\n",
    "    x, x_tau, t_values, tau_values, alpha_values  = x.unsqueeze(1).float().cuda(), x_tau.unsqueeze(1).float().cuda(), t_values.unsqueeze(1).float().cuda(), tau_values.unsqueeze(1).float().cuda(), alpha_values.unsqueeze(1).float().cuda() \n",
    "\n",
    "    x_hat, x_hat_tau, mean, log_var, z_tau, z_ = model(x, tau_values, alpha_values)\n",
    "\n",
    "    # VISUALIZATION\n",
    "    index = np.random.randint(0, len(x))\n",
    "\n",
    "    plt.figure(figsize = (14, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(x[index, :, :, :].squeeze().cpu().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Truth\")\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(x_hat[index, :, :, :].squeeze().cpu().detach().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Model Reconstruction\")\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(x_tau[index, :, :, :].squeeze().cpu().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Truth: Forecast State\")\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(x_hat_tau[index, :, :, :].squeeze().cpu().detach().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Model Prediction\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeaec53-5df5-4117-9572-241f1e1d3388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046dfba7-31e4-404c-8d18-f13a42ab571e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
