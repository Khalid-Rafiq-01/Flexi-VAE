{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64972205-0661-4f0e-8f2c-e25d615768bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Earlier ---------------------------------------------------------------------------------\n",
    "\n",
    "# Earlier used -> ReDataset(AdvectionDiffussionDataset), Re_values(alpha_values), prepare_Re_dataset(prepare_adv_diff_dataset), Re_range(alpha_range)\n",
    "# Re(alpha), re_interpolation_span(alpha_interpolation_span), re_extrapolation_left_span(alpha_extrapolation_left_span)\n",
    "# re_extrapolation_right_span(alpha_extrapolation_right_span), Re_interval_split(alpha_interval_split), Re_train_ranges(alpha_train_ranges)\n",
    "# re_train_range(alpha_train_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885938a-42aa-447b-af83-6ddc246de2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "import wandb\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "from data import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config, load_config\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from model_io import load_model, save_model\n",
    "\n",
    "from data import load_from_path, prepare_adv_diff_dataset, AdvectionDiffussionDataset, get_train_val_test_folds, IntervalSplit, exact_solution\n",
    "\n",
    "# We we define all our model here:\n",
    "from new_model import Encoder, Decoder, Propagator_concat as Propagator, Model, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd63f1-f861-4d8d-a8b2-06809d9fbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "def get_model(latent_dim):\n",
    "    encoder = Encoder(latent_dim)\n",
    "    decoder  = Decoder(latent_dim)  # Decoder for x(t)\n",
    "    propagator = Propagator_concat(latent_dim) # z(t) --> z(t+tau)\n",
    "    model = Model(encoder, decoder, propagator)\n",
    "    return model\n",
    "\n",
    "def get_data_loader(dataset, batch_size):\n",
    "    data = list(zip(dataset.X, dataset.X_tau, dataset.t_values, dataset.tau_values, dataset.alpha_values))\n",
    "    data = data[: len(data) - len(data) % batch_size]\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    \n",
    "def plot_prediction(x, x_tau, x_hat, x_hat_tau, tau, alpha):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 6))  # 2 rows, 2 columns\n",
    "\n",
    "    # Plot each field\n",
    "    axes[0, 0].imshow(x.cpu().squeeze().numpy(), cmap=\"jet\")\n",
    "    axes[0, 0].set_title(\"x (Original)\", fontsize=12)\n",
    "\n",
    "    axes[0, 1].imshow(x_tau.cpu().squeeze().numpy(), cmap=\"jet\")\n",
    "    axes[0, 1].set_title(\"x_tau (Ground Truth)\", fontsize=12)\n",
    "\n",
    "    axes[1, 0].imshow(x_hat.cpu().squeeze().detach().numpy(), cmap=\"jet\")\n",
    "    axes[1, 0].set_title(\"x_hat (Reconstruction)\", fontsize=12)\n",
    "\n",
    "    axes[1, 1].imshow(x_hat_tau.cpu().squeeze().detach().numpy(), cmap=\"jet\")\n",
    "    axes[1, 1].set_title(\"x_hat_tau (Predicted)\", fontsize=12)\n",
    "\n",
    "    # Add a common title for the figure\n",
    "    fig.suptitle(f\"Tau: {tau.item()}, Re: {alpha.item():.2f}\", fontsize=12)\n",
    "\n",
    "    # Remove axes for clean visualization\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "    \n",
    "    \n",
    "def validate(config: Config, model, val_loader, step):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in val_loader:\n",
    "        x, x_tau, t, tau, alpha = batch\n",
    "        x, x_tau, t, tau, alpha = x.cuda().float().unsqueeze(1), x_tau.cuda().float().unsqueeze(1), t.cuda().float().unsqueeze(1), tau.cuda().float().unsqueeze(1), alpha.cuda().float().unsqueeze(1)\n",
    "        x_hat, x_hat_tau, mean, log_var, z_tau, _ = model(x, tau, alpha)\n",
    "        reconstruction_loss, reconstruction_loss_tau, KLD = loss_function(x, x_tau, x_hat, x_hat_tau, mean, log_var)\n",
    "        loss = reconstruction_loss + config.gamma * reconstruction_loss_tau + config.beta * KLD\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # plot the last sample\n",
    "    fig, ax = plot_prediction(x[0], x_tau[0], x_hat[0], x_hat_tau[0], tau[0], alpha[0])\n",
    "    wandb.log({'plot_val': fig}, step=step)\n",
    "    # plt.close(fig)\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def train(config: Config):\n",
    "    os.makedirs(config.save_dir, exist_ok=True)\n",
    "    # model id name + timestamp + random uuid\n",
    "    model_id = f'{config.name}_{datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}_{str(uuid.uuid4()).split(\"-\")[0]}'\n",
    "    save_path = os.path.join(config.save_dir, model_id)\n",
    "    conf = asdict(config)\n",
    "    conf['save_path'] = save_path\n",
    "\n",
    "    wandb.init(project='FlexiPropagator_2D', config=conf)\n",
    "\n",
    "    \n",
    "    model = get_model(config.latent_dim)\n",
    "    optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    logger.info('Model and optimizer created')\n",
    "    logger.info('Loading data')\n",
    "    # tau_range = (int(config.tau_left_fraction * config.num_time_steps), int(config.tau_right_fraction * config.num_time_steps))\n",
    "    # dataset_train, dataset_val, Re_interval_split, tau_interval_split = get_train_val_test_folds((1000, 3000),\n",
    "    #                                                                                          tau_range,\n",
    "    #                                                                                       n_samples_train=config.n_samples_train)\n",
    "    \n",
    "    dataset_train, dataset_val, alpha_interval_split, tau_interval_split = load_from_path(\"data\")\n",
    "    logger.info('Data loaded')\n",
    "    train_loader = get_data_loader(dataset_train, config.batch_size)\n",
    "    val_loader = get_data_loader(dataset_val, config.batch_size)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.lr, epochs=config.num_epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "\n",
    "    pbar = tqdm(range(total_steps), total=total_steps, desc='Training')\n",
    "\n",
    "    step = 0\n",
    "    val_every_int = int(config.val_every * len(train_loader))\n",
    "    plot_train_every_int = int(config.plot_train_every * len(train_loader))\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    logger.info('Starting training')\n",
    "    for epoch in range(config.num_epochs):\n",
    "        wandb.log({'epoch': epoch}, step=step)\n",
    "        for batch in train_loader:\n",
    "            x, x_tau, t, tau, alpha = batch\n",
    "            x, x_tau, t, tau, alpha = x.cuda().float().unsqueeze(1), x_tau.cuda().float().unsqueeze(1), t.cuda().float().unsqueeze(1), tau.cuda().float().unsqueeze(1), alpha.cuda().float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, x_hat_tau, mean, log_var, z_tau, _ = model(x, tau, alpha)\n",
    "            reconstruction_loss, reconstruction_loss_tau, KLD = loss_function(x, x_tau, x_hat, x_hat_tau, mean, log_var)\n",
    "            loss = reconstruction_loss + config.gamma * reconstruction_loss_tau + config.beta * KLD\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            if step % 100 == 0:\n",
    "                wandb.log({'loss': loss.item(), 'reconstruction_loss': reconstruction_loss.item(), 'reconstruction_loss_tau': reconstruction_loss_tau.item(), 'KLD': KLD.item(), 'lr': scheduler.get_last_lr()[0]}, step=step)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # if step % plot_train_every_int == 0:\n",
    "                #     # plot train \n",
    "                #     fig, ax = plot_prediction(x[0], x_tau[0], x_hat[0], x_hat_tau[0], tau[0], re[0])\n",
    "                #     wandb.log({'plot': fig}, step=step)\n",
    "                #     # plt.close(fig)\n",
    "\n",
    "\n",
    "                if step % val_every_int == 0:\n",
    "                    val_loss = validate(config, model, val_loader, step=step)\n",
    "                    wandb.log({'val_loss': val_loss}, step=step)\n",
    "\n",
    "                    # save latest\n",
    "                    # torch.save(model.state_dict(), 'model_latest.pt')\n",
    "                    save_model(save_path + '_latest.pt', model, tau_interval_split, alpha_interval_split, config)\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        save_model(save_path + '_best.pt', model, tau_interval_split, alpha_interval_split, config)\n",
    "                        \n",
    "                    model.train()\n",
    "            step += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--latent_dim', type=int, default=3)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--lr', type=float, default=3e-4)\n",
    "    parser.add_argument('--num_epochs', type=int, default=50)\n",
    "    parser.add_argument('--n_samples_train', type=int, default=25000)\n",
    "    parser.add_argument('--gamma', type=float, default=2.5)\n",
    "    parser.add_argument('--beta', type=float, default=1e-4)\n",
    "    parser.add_argument('--val_every', type=float, default=0.25)\n",
    "    parser.add_argument('--plot_train_every', type=float, default=0.25)\n",
    "    \n",
    "\n",
    "    parser.add_argument('--config', type=str, required=False)\n",
    "    args = parser.parse_args()\n",
    "    if args.config:\n",
    "        config = load_config(args.config)\n",
    "    else:\n",
    "        conf = dict(vars(args))\n",
    "        conf.pop('config')\n",
    "        config = Config(**conf)\n",
    "\n",
    "    train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
