{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28b4a8-e4d4-4c38-8839-44fd844ee6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val, alpha_interval_split, tau_interval_split = load_from_path(\"data\")\n",
    "X, X_tau, t_values, tau_values, alpha_values = dataset_train.X, dataset_train.X_tau, dataset_train.t_values, dataset_train.tau_values, dataset_train.alpha_values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cf7a9-383c-404e-8386-f286c36be702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just an autoencoder:\n",
    "import torch.nn as nn\n",
    "# Normalization Layer for Conv2D\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, num_channels, num_groups=4):\n",
    "        super(Norm, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups, num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "# Encoder using Conv2D\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: (batch_size, 1, 256, 256)\n",
    "            nn.Conv2d(1, 32, kernel_size=2, stride=2, padding=0),  # (batch_size, 64, 128, 128)\n",
    "            nn.GELU(),\n",
    "            Norm(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0),  # (batch_size, 128, 64, 64)\n",
    "            nn.GELU(),\n",
    "            Norm(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),  # (batch_size, 256, 32, 32)\n",
    "            nn.GELU(),\n",
    "            Norm(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),  # (batch_size, 512, 16, 16)\n",
    "            nn.GELU(),\n",
    "            Norm(256),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),  # (batch_size, 512, 8, 8)\n",
    "            nn.GELU(),\n",
    "            Norm(512),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66d2d9-d520-473c-b92d-db47ceaaba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.rand(size = (32, 1, 128, 128), dtype = torch.float32)\n",
    "print(x_1.shape)\n",
    "\n",
    "encoder = Encoder(latent_dim = 3)\n",
    "mean, log_var = encoder(x_1)\n",
    "\n",
    "print(\"mean shape\",mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4423fda-bc4f-4a37-992f-342818297798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Fully connected layer to transform the latent vector back to the shape (batch_size, 512, 8, 8)\n",
    "        self.fc = nn.Linear(latent_dim, 512 * 4 * 4)\n",
    "\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(256),\n",
    "\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(128),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(64),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            Norm(32),\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Transform the latent vector to match the shape of the feature maps\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 512, 4, 4)  # Reshape to (batch_size, 512, 4, 4)\n",
    "        x = self.deconv_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder # decoder for x(t)\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "\n",
    "        # Reconstruction\n",
    "        x_hat = self.decoder(z)  # Reconstruction of x(t)\n",
    "        return x_hat.squeeze(), mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2669b-1a22-45d5-825f-4f42911962b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size = (32, 1, 128, 128), dtype = torch.float32)\n",
    "print(x.shape)\n",
    "\n",
    "encoder = Encoder(latent_dim = 3)\n",
    "decoder = Decoder(latent_dim = 3)\n",
    "model = Model(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd0eb6-79db-4d89-9d9d-8bd575b4f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat, mean, log_var = model(x)\n",
    "print(x_hat.shape, mean.shape, log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b47c5-e33a-4a75-b8bb-65f5c660c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a6784-56c1-46e8-ad86-13f929f60f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "X_data = X[: len(X) - len(X) % batch_size]\n",
    "train_loader = DataLoader(X_data, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5e966-fb0e-4051-8683-ffeeff83f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0, 32)\n",
    "for data in train_loader:\n",
    "    print(data.shape)\n",
    "    plt.imshow(data[index, :, :], cmap = \"jet\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfe769-9c95-46ec-9a48-d3657c1ad6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF(x, x_hat, mean, log_var):\n",
    "    RL_1 = nn.MSELoss()(x, x_hat)\n",
    "    KLD = torch.mean(-0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), dim=1))\n",
    "    return RL_1, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617de32-1d70-4d4a-af4e-2fbec92d1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train().cuda()\n",
    "for data in train_loader:\n",
    "    x = data.unsqueeze(1).float().cuda()\n",
    "    print(\"Input data shape: \", x.shape)\n",
    "    x_hat, mean, log_var = model(x)\n",
    "    print(\"Output data shape: \", x_hat.shape)\n",
    "    print(\"Mean shape: \", mean.shape)\n",
    "    print(\"Logvar shape: \", log_var.shape)\n",
    "    \n",
    "    RL_1, KLD = LF(x.squeeze(), x_hat, mean, log_var)\n",
    "    print(RL_1, KLD)\n",
    "    overall_loss = RL_1 + 2*KLD\n",
    "    print(\"Overall Loss: \", overall_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d68de-0323-4664-9ba5-de8f86b6c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "model = model.train().cuda()  # Putting model on GPU and setting it to train mode\n",
    "losses = []  # To track the training loss after each epoch\n",
    "\n",
    "optimizer = Adam(model.parameters(), 1e-3)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # Accumulate loss for the epoch\n",
    "    for data in train_loader:\n",
    "        x = data.unsqueeze(1).float().cuda()  # Ensure the input tensor is on GPU and has the correct shape\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "        \n",
    "        # Forward pass\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        \n",
    "        # Compute loss\n",
    "        RL_1, KLD = LF(x.squeeze(), x_hat, mean, log_var)  # Custom loss function\n",
    "        overall_loss = RL_1 + 0.00001 * KLD\n",
    "        \n",
    "        # Backward pass\n",
    "        overall_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate batch loss\n",
    "        epoch_loss += overall_loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # Print loss for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# After training, plot the loss curve if needed\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aeba04-e3e5-48ac-8587-b870e2c02034",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat, mean, log_var = model(x)\n",
    "print(x_hat.shape)\n",
    "\n",
    "index = np.random.randint(0, len(x_hat))\n",
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(x[index, :, :, :].squeeze().cpu().numpy(), cmap = \"jet\")\n",
    "plt.title(\"Original Field\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(x_hat[index, :, :].squeeze().cpu().detach().numpy(), cmap = \"jet\")\n",
    "plt.title(\"Reconstruction\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "error = (x[index, :, :, :].squeeze().cpu().numpy() - x_hat[index, :, :].squeeze().cpu().detach().numpy())**2\n",
    "plt.imshow(error, cmap=\"jet\", vmin=0, vmax=1e-3)  # Set colorbar limits\n",
    "plt.title(\"Error\")\n",
    "plt.colorbar(fraction=0.04)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee51db-afac-492d-b94a-e0c7e93876b8",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ee5b0-42cf-4e5e-9496-d19526eb4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_tau_val, t_values_val, tau_values_val, alpha_values_val = dataset_val.X, dataset_val.X_tau, dataset_val.t_values, dataset_val.tau_values, dataset_val.alpha_values\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91621538-e404-4043-9145-725aa376b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0, 9000)\n",
    "x = torch.tensor(X_val[index, :, :][None, None, :, :], dtype = torch.float32).cuda() #Batch_size, input_dim needs to added as dimensions\n",
    "print(x.shape)\n",
    "\n",
    "x_hat, mean, log_var = model(x)\n",
    "print(x_hat.shape)\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(x.squeeze().cpu().numpy(), cmap = \"jet\")\n",
    "plt.title(\"Original Field\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(x_hat.squeeze().cpu().detach().numpy(), cmap = \"jet\")\n",
    "plt.title(\"Reconstruction\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "error = (x.squeeze().cpu().numpy() - x_hat.squeeze().cpu().detach().numpy())**2\n",
    "plt.imshow(error, cmap=\"jet\", vmin=0, vmax=1e-3)  # Set colorbar limits\n",
    "plt.title(\"Error\")\n",
    "plt.colorbar(fraction=0.04)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
